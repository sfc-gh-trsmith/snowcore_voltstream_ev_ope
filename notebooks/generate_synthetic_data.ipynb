{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "title_and_objectives"
   },
   "source": [
    "# Synthetic Data Generation for VoltStream EV OPE Demo\n",
    "\n",
    "## Business Objective\n",
    "\n",
    "Generate causally-linked synthetic data that demonstrates the OEE vs OPE gap problem in EV manufacturing. The data tells a compelling story where environmental conditions (low humidity â†’ high dust) cause AGV sensor failures, leading to material starvation and OPE drops.\n",
    "\n",
    "## Technical Approach\n",
    "\n",
    "Using Snowpark Python, we generate data directly in Snowflake using `session.write_pandas()` for efficient bulk loading. This approach is faster than CSV file generation and upload.\n",
    "\n",
    "## Data Story\n",
    "\n",
    "- **Months 1-2 (Baseline)**: Steady production, normal humidity (~45%), low dust (~10 Âµg/mÂ³), 99.9% AGV reliability\n",
    "- **Month 3 Week 2 (Crisis)**: Humidity drops to 25% â†’ Dust spikes to 35 Âµg/mÂ³ â†’ AGV-ERR-99 sensor errors â†’ Line starvation â†’ OPE drops to 60%\n",
    "\n",
    "## Notebook Structure\n",
    "\n",
    "| Section | Purpose |\n",
    "|---------|--------|\n",
    "| 1. Environment Setup | Import packages, connect to Snowflake |\n",
    "| 2. Configuration | Define date ranges, constants, parameters |\n",
    "| 3. Helper Functions | Utility functions for data generation |\n",
    "| 4. Generate Dimension Data | Create ASSET and PRODUCT dimensions |\n",
    "| 5. Generate Environmental Data | Humidity, dust, temperature sensors |\n",
    "| 6. Generate AGV Telemetry | AGV positions and error codes |\n",
    "| 7. Generate Equipment States | Downtime and starvation events |\n",
    "| 8. Generate Production Orders | Work orders with actual quantities |\n",
    "| 9. Generate Material Documents | Inventory movements |\n",
    "| 10. Verification | Row counts and data quality checks |\n",
    "\n",
    "## Output\n",
    "\n",
    "- `ATOMIC.ASSET` - 25 assets (5 lines + 20 AGVs)\n",
    "- `ATOMIC.PRODUCT` - 6 products (battery packs, modules, cells)\n",
    "- `RAW.ENV_SENSOR_STREAM_STAGE` - ~1.5M environmental readings\n",
    "- `RAW.AGV_TELEMATICS_LOG_CDC` - ~2.6M AGV telemetry records\n",
    "- `RAW.EQUIPMENT_STATE_LOG_CDC` - ~11K equipment state events\n",
    "- `RAW.PROD_ORDER_HEADER_CDC` - ~1.4K production orders\n",
    "- `RAW.MATERIAL_DOCUMENT_CDC` - ~7K material movements\n",
    "\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "After completing this notebook, you will understand:\n",
    "1. How to generate causally-linked synthetic manufacturing data\n",
    "2. The relationship between environmental conditions and equipment failures\n",
    "3. How to use Snowpark `write_pandas()` for efficient bulk loading\n",
    "4. Patterns for creating time-series data with controlled anomaly periods\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- **Python**: Familiarity with pandas, numpy, and datetime operations\n",
    "- **Snowflake**: Basic understanding of Snowpark sessions and table operations\n",
    "- **Domain**: General understanding of manufacturing OEE/OPE concepts\n",
    "\n",
    "## Idempotency\n",
    "\n",
    "This notebook is **idempotent** - it can be executed multiple times safely.\n",
    "All tables are truncated at the start before data generation begins.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "environment_setup_header"
   },
   "source": [
    "## 1. Environment Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "import_packages"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IMPORTS\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import uuid\n",
    "from datetime import datetime, timedelta\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "\n",
    "# Visualization imports\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"âœ… All packages imported\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "snowflake_session_setup"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SNOWFLAKE SESSION\n",
    "# =============================================================================\n",
    "\n",
    "session = get_active_session()\n",
    "\n",
    "# Verify connection\n",
    "result = session.sql(\"SELECT CURRENT_DATABASE(), CURRENT_SCHEMA(), CURRENT_WAREHOUSE()\").collect()\n",
    "print(f\"âœ… Connected to Snowflake\")\n",
    "print(f\"   Database: {result[0][0]}\")\n",
    "print(f\"   Schema: {result[0][1]}\")\n",
    "print(f\"   Warehouse: {result[0][2]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cleanup_tables"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CLEANUP - Truncate tables to ensure clean state on re-run\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Truncating tables for clean data generation...\")\n",
    "\n",
    "# Truncate RAW tables (will be populated by this notebook)\n",
    "session.sql(\"TRUNCATE TABLE IF EXISTS RAW.ENV_SENSOR_STREAM_STAGE\").collect()\n",
    "session.sql(\"TRUNCATE TABLE IF EXISTS RAW.AGV_TELEMATICS_LOG_CDC\").collect()\n",
    "session.sql(\"TRUNCATE TABLE IF EXISTS RAW.EQUIPMENT_STATE_LOG_CDC\").collect()\n",
    "session.sql(\"TRUNCATE TABLE IF EXISTS RAW.PROD_ORDER_HEADER_CDC\").collect()\n",
    "session.sql(\"TRUNCATE TABLE IF EXISTS RAW.MATERIAL_DOCUMENT_CDC\").collect()\n",
    "\n",
    "# Truncate ATOMIC dimension tables (will be populated by this notebook)\n",
    "session.sql(\"TRUNCATE TABLE IF EXISTS ATOMIC.ASSET\").collect()\n",
    "session.sql(\"TRUNCATE TABLE IF EXISTS ATOMIC.PRODUCT\").collect()\n",
    "\n",
    "print(\"âœ… Tables truncated - ready for fresh data generation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "configuration_header"
   },
   "source": [
    "## 2. Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "define_configuration"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Date range: 3 months of data\n",
    "START_DATE = datetime(2024, 10, 1)\n",
    "END_DATE = datetime(2024, 12, 31)\n",
    "\n",
    "# Crisis period: Month 3, Week 2 (Dec 9-11, 2024)\n",
    "CRISIS_START = datetime(2024, 12, 9)\n",
    "CRISIS_END = datetime(2024, 12, 11, 23, 59, 59)\n",
    "\n",
    "# Production lines\n",
    "PRODUCTION_LINES = ['LINE_1', 'LINE_2', 'LINE_3', 'LINE_4', 'LINE_5']\n",
    "\n",
    "# AGVs (20 total)\n",
    "AGVS = [f'AGV_{i:03d}' for i in range(1, 21)]\n",
    "\n",
    "# Zones\n",
    "ZONES = ['ZONE_A', 'ZONE_B', 'ZONE_C', 'ZONE_D']\n",
    "\n",
    "# Shifts\n",
    "SHIFTS = ['SHIFT_1', 'SHIFT_2', 'SHIFT_3']\n",
    "\n",
    "# Products (Battery hierarchy)\n",
    "PRODUCTS = [\n",
    "    {'sku': 'BP-100', 'name': 'Battery Pack 100kWh', 'category': 'BATTERY_PACK'},\n",
    "    {'sku': 'BP-150', 'name': 'Battery Pack 150kWh', 'category': 'BATTERY_PACK'},\n",
    "    {'sku': 'MOD-20', 'name': 'Battery Module 20kWh', 'category': 'MODULE'},\n",
    "    {'sku': 'MOD-25', 'name': 'Battery Module 25kWh', 'category': 'MODULE'},\n",
    "    {'sku': 'CELL-2170', 'name': '2170 Cell', 'category': 'CELL'},\n",
    "    {'sku': 'CELL-4680', 'name': '4680 Cell', 'category': 'CELL'},\n",
    "]\n",
    "\n",
    "print(\"âœ… Configuration loaded\")\n",
    "print(f\"   Date range: {START_DATE.date()} to {END_DATE.date()} ({(END_DATE - START_DATE).days + 1} days)\")\n",
    "print(f\"   Crisis period: {CRISIS_START.date()} to {CRISIS_END.date()}\")\n",
    "print(f\"   Production lines: {len(PRODUCTION_LINES)}\")\n",
    "print(f\"   AGVs: {len(AGVS)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "helper_functions_header"
   },
   "source": [
    "## 3. Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "define_helper_functions"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def is_crisis_period(timestamp: datetime) -> bool:\n",
    "    \"\"\"Check if timestamp falls within the crisis period.\"\"\"\n",
    "    return CRISIS_START <= timestamp <= CRISIS_END\n",
    "\n",
    "\n",
    "def get_shift_for_hour(hour: int) -> str:\n",
    "    \"\"\"Get shift ID based on hour of day.\"\"\"\n",
    "    if 6 <= hour < 14:\n",
    "        return 'SHIFT_1'\n",
    "    elif 14 <= hour < 22:\n",
    "        return 'SHIFT_2'\n",
    "    else:\n",
    "        return 'SHIFT_3'\n",
    "\n",
    "\n",
    "def generate_uuid() -> str:\n",
    "    \"\"\"Generate a UUID string.\"\"\"\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "\n",
    "def convert_timestamps(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Convert all datetime columns to string format for Snowflake compatibility.\"\"\"\n",
    "    df = df.copy()\n",
    "    for col in df.columns:\n",
    "        # Check if column contains datetime objects\n",
    "        if len(df) > 0:\n",
    "            non_null = df[col].dropna()\n",
    "            if len(non_null) > 0:\n",
    "                sample = non_null.iloc[0]\n",
    "                if isinstance(sample, datetime):\n",
    "                    # Use pd.notna() to properly handle NaT values\n",
    "                    df[col] = df[col].apply(\n",
    "                        lambda x: x.strftime('%Y-%m-%d %H:%M:%S') if pd.notna(x) else None\n",
    "                    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def write_to_snowflake(df: pd.DataFrame, table_name: str, schema: str = 'RAW'):\n",
    "    \"\"\"\n",
    "    Write DataFrame to Snowflake table using write_pandas.\n",
    "    FAIL-FAST: Raises exception on failure.\n",
    "    \"\"\"\n",
    "    # Get database name and strip any quotes\n",
    "    db_name = session.get_current_database().replace('\"', '')\n",
    "    full_table_name = f\"{schema}.{table_name}\"\n",
    "    \n",
    "    # Convert to uppercase column names for Snowflake compatibility\n",
    "    df.columns = [col.upper() for col in df.columns]\n",
    "    \n",
    "    # Convert datetime columns to string format for Snowflake compatibility\n",
    "    df = convert_timestamps(df)\n",
    "    \n",
    "    # Write to Snowflake (append mode - tables are truncated at start of notebook)\n",
    "    snowpark_df = session.write_pandas(\n",
    "        df,\n",
    "        table_name=table_name,\n",
    "        database=db_name,\n",
    "        schema=schema,\n",
    "        auto_create_table=False,\n",
    "        overwrite=False\n",
    "    )\n",
    "    \n",
    "    print(f\"   âœ… Wrote {len(df):,} rows to {full_table_name}\")\n",
    "    return snowpark_df\n",
    "\n",
    "\n",
    "print(\"âœ… Helper functions defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "dimension_data_header"
   },
   "source": [
    "## 4. Generate Dimension Data\n",
    "\n",
    "Create master data for assets (production lines and AGVs) and products.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "generate_asset_dimension"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GENERATE ASSET DIMENSION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Generating ASSET dimension...\")\n",
    "\n",
    "asset_records = []\n",
    "asset_id = 1\n",
    "\n",
    "# Production lines\n",
    "for line in PRODUCTION_LINES:\n",
    "    asset_records.append({\n",
    "        'ASSET_ID': asset_id,\n",
    "        'ASSET_CODE': line,\n",
    "        'ASSET_NAME': f'Production Line {line[-1]}',\n",
    "        'ASSET_DESCRIPTION': f'Battery pack assembly line {line[-1]}',\n",
    "        'ASSET_TYPE': 'PRODUCTION_LINE',\n",
    "        'ASSET_CLASS': 'ASSEMBLY',\n",
    "        'ASSET_STATUS': 'ACTIVE',\n",
    "        'SITE_ID': 1,\n",
    "        'PRODUCTION_LINE_ID': line,\n",
    "        'ZONE_ID': f'ZONE_{chr(64 + int(line[-1]))}',\n",
    "        'MANUFACTURER': 'Siemens',\n",
    "        'MODEL_NUMBER': 'SIMATIC-5000'\n",
    "    })\n",
    "    asset_id += 1\n",
    "\n",
    "# AGVs\n",
    "for agv in AGVS:\n",
    "    asset_records.append({\n",
    "        'ASSET_ID': asset_id,\n",
    "        'ASSET_CODE': agv,\n",
    "        'ASSET_NAME': f'Automated Guided Vehicle {agv[-3:]}',\n",
    "        'ASSET_DESCRIPTION': 'Material transport AGV with LiDAR navigation',\n",
    "        'ASSET_TYPE': 'AGV',\n",
    "        'ASSET_CLASS': 'TRANSPORT',\n",
    "        'ASSET_STATUS': 'ACTIVE',\n",
    "        'SITE_ID': 1,\n",
    "        'PRODUCTION_LINE_ID': None,\n",
    "        'ZONE_ID': random.choice(ZONES),\n",
    "        'MANUFACTURER': 'KUKA',\n",
    "        'MODEL_NUMBER': 'KMR-iiwa'\n",
    "    })\n",
    "    asset_id += 1\n",
    "\n",
    "df_assets = pd.DataFrame(asset_records)\n",
    "\n",
    "# Write to ATOMIC.ASSET\n",
    "write_to_snowflake(df_assets, 'ASSET', 'ATOMIC')\n",
    "print(f\"   Total assets: {len(df_assets)} ({len(PRODUCTION_LINES)} lines + {len(AGVS)} AGVs)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "generate_product_dimension"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GENERATE PRODUCT DIMENSION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Generating PRODUCT dimension...\")\n",
    "\n",
    "product_records = []\n",
    "for i, product in enumerate(PRODUCTS, 1):\n",
    "    product_records.append({\n",
    "        'PRODUCT_ID': i,\n",
    "        'PRODUCT_CODE': product['sku'],\n",
    "        'PRODUCT_NAME': product['name'],\n",
    "        'PRODUCT_DESCRIPTION_SHORT': f\"High-performance {product['name'].lower()} for electric vehicles\",\n",
    "        'PRODUCT_CATEGORY': product['category'],\n",
    "        'PRODUCT_FAMILY': 'EV_BATTERY',\n",
    "        'PRODUCT_LINE': 'VOLTSTREAM',\n",
    "        'UNIT_OF_MEASURE': 'EA',\n",
    "        'STANDARD_COST': round(random.uniform(100, 5000), 2),\n",
    "        'STANDARD_CYCLE_TIME_SEC': random.randint(30, 120)\n",
    "    })\n",
    "\n",
    "df_products = pd.DataFrame(product_records)\n",
    "\n",
    "# Write to ATOMIC.PRODUCT\n",
    "write_to_snowflake(df_products, 'PRODUCT', 'ATOMIC')\n",
    "print(f\"   Total products: {len(df_products)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "env_sensor_header"
   },
   "source": [
    "## 5. Generate Environmental Sensor Data\n",
    "\n",
    "Environmental readings at 5-minute intervals. During the crisis period:\n",
    "- Humidity drops from ~45% to ~25%\n",
    "- Dust (PM2.5) increases from ~10 to ~35 Âµg/mÂ³\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "generate_env_sensor_data"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GENERATE ENVIRONMENTAL SENSOR DATA\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Generating ENV_SENSOR_STREAM_STAGE...\")\n",
    "print(\"   This may take a few minutes for ~1.5M rows...\")\n",
    "\n",
    "# Reset seed for reproducibility\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "records = []\n",
    "current_time = START_DATE\n",
    "reading_interval = timedelta(minutes=5)\n",
    "sensor_types = ['HUMIDITY', 'PM25', 'TEMPERATURE']\n",
    "batch_size = 100000\n",
    "total_rows = 0\n",
    "\n",
    "while current_time <= END_DATE:\n",
    "    is_crisis = is_crisis_period(current_time)\n",
    "    \n",
    "    for zone in ZONES:\n",
    "        for line in PRODUCTION_LINES:\n",
    "            for sensor_type in sensor_types:\n",
    "                sensor_id = f\"{zone}_{line}_{sensor_type}_01\"\n",
    "                \n",
    "                if sensor_type == 'HUMIDITY':\n",
    "                    if is_crisis:\n",
    "                        value = max(15, min(40, np.random.normal(25, 3)))\n",
    "                    else:\n",
    "                        value = max(30, min(60, np.random.normal(45, 5)))\n",
    "                    unit = '%'\n",
    "                elif sensor_type == 'PM25':\n",
    "                    if is_crisis:\n",
    "                        base_dust = 50 - 25 + np.random.normal(10, 5)\n",
    "                        value = max(5, min(50, base_dust))\n",
    "                    else:\n",
    "                        value = max(2, min(20, np.random.normal(10, 2)))\n",
    "                    unit = 'Âµg/mÂ³'\n",
    "                else:  # TEMPERATURE\n",
    "                    value = np.random.normal(22, 1.5)\n",
    "                    unit = 'Â°C'\n",
    "                \n",
    "                records.append({\n",
    "                    'READING_ID': generate_uuid(),\n",
    "                    'SENSOR_ID': sensor_id,\n",
    "                    'SENSOR_TYPE': sensor_type,\n",
    "                    'ZONE_ID': zone,\n",
    "                    'PRODUCTION_LINE_ID': line,\n",
    "                    'READING_TIMESTAMP': current_time,\n",
    "                    'METRIC_NAME': sensor_type,\n",
    "                    'METRIC_VALUE': round(value, 3),\n",
    "                    'UNIT_OF_MEASURE': unit,\n",
    "                    'QUALITY_FLAG': 'GOOD'\n",
    "                })\n",
    "    \n",
    "    # Write in batches to avoid memory issues\n",
    "    if len(records) >= batch_size:\n",
    "        df = pd.DataFrame(records)\n",
    "        write_to_snowflake(df, 'ENV_SENSOR_STREAM_STAGE', 'RAW')\n",
    "        total_rows += len(records)\n",
    "        records = []\n",
    "    \n",
    "    current_time += reading_interval\n",
    "\n",
    "# Write remaining records\n",
    "if records:\n",
    "    df = pd.DataFrame(records)\n",
    "    write_to_snowflake(df, 'ENV_SENSOR_STREAM_STAGE', 'RAW')\n",
    "    total_rows += len(records)\n",
    "\n",
    "print(f\"âœ… Total ENV_SENSOR_STREAM_STAGE rows: {total_rows:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "agv_telemetry_header"
   },
   "source": [
    "## 6. Generate AGV Telemetry Data\n",
    "\n",
    "AGV position and error data at 1-minute intervals. During crisis:\n",
    "- 15% error probability when dust is high\n",
    "- AGV-ERR-99 (optical sensor obscured) is the primary error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "generate_agv_telemetry"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GENERATE AGV TELEMETRY DATA\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Generating AGV_TELEMATICS_LOG_CDC...\")\n",
    "print(\"   This may take several minutes for ~2.6M rows...\")\n",
    "\n",
    "# Reset seed for reproducibility\n",
    "random.seed(RANDOM_SEED + 1)\n",
    "np.random.seed(RANDOM_SEED + 1)\n",
    "\n",
    "records = []\n",
    "current_time = START_DATE\n",
    "reading_interval = timedelta(minutes=1)\n",
    "batch_size = 100000\n",
    "total_rows = 0\n",
    "\n",
    "while current_time <= END_DATE:\n",
    "    is_crisis = is_crisis_period(current_time)\n",
    "    \n",
    "    for agv_id in AGVS:\n",
    "        x = random.uniform(0, 500)\n",
    "        y = random.uniform(0, 300)\n",
    "        zone = random.choice(ZONES)\n",
    "        route = f\"ROUTE_{random.randint(1, 10):02d}\"\n",
    "        velocity = random.uniform(0.5, 2.0) if random.random() > 0.1 else 0\n",
    "        battery = random.uniform(20, 100)\n",
    "        \n",
    "        error_code = None\n",
    "        error_severity = None\n",
    "        error_message = None\n",
    "        \n",
    "        if is_crisis:\n",
    "            if random.random() < 0.15:\n",
    "                error_code = 'AGV-ERR-99'\n",
    "                error_severity = 'ERROR'\n",
    "                error_message = 'Optical sensor obscured - reduced visibility detected'\n",
    "        else:\n",
    "            if random.random() < 0.001:\n",
    "                error_code = random.choice(['AGV-ERR-01', 'AGV-ERR-42', 'AGV-ERR-55'])\n",
    "                error_severity = 'WARNING'\n",
    "                error_message = 'Minor operational issue detected'\n",
    "        \n",
    "        records.append({\n",
    "            'MSG_ID': generate_uuid(),\n",
    "            'AGV_ID': agv_id,\n",
    "            'EVENT_TIMESTAMP': current_time,\n",
    "            'X_COORD': round(x, 3),\n",
    "            'Y_COORD': round(y, 3),\n",
    "            'Z_COORD': 0,\n",
    "            'ZONE_ID': zone,\n",
    "            'ROUTE_SEGMENT': route,\n",
    "            'VELOCITY': round(velocity, 3),\n",
    "            'BATTERY_LEVEL': round(battery, 2),\n",
    "            'PAYLOAD_ID': f'BATCH_{random.randint(1000, 9999)}' if random.random() > 0.3 else None,\n",
    "            'ERROR_CODE': error_code,\n",
    "            'ERROR_SEVERITY': error_severity,\n",
    "            'ERROR_MESSAGE': error_message,\n",
    "            '_CDC_OPERATION': 'INSERT',\n",
    "            '_CDC_TIMESTAMP': current_time,\n",
    "            '_CDC_SEQUENCE': int(current_time.timestamp() * 1000),\n",
    "            '_CDC_SOURCE_SYSTEM': 'SIEMENS_MES'\n",
    "        })\n",
    "    \n",
    "    if len(records) >= batch_size:\n",
    "        df = pd.DataFrame(records)\n",
    "        write_to_snowflake(df, 'AGV_TELEMATICS_LOG_CDC', 'RAW')\n",
    "        total_rows += len(records)\n",
    "        records = []\n",
    "    \n",
    "    current_time += reading_interval\n",
    "\n",
    "if records:\n",
    "    df = pd.DataFrame(records)\n",
    "    write_to_snowflake(df, 'AGV_TELEMATICS_LOG_CDC', 'RAW')\n",
    "    total_rows += len(records)\n",
    "\n",
    "print(f\"âœ… Total AGV_TELEMATICS_LOG_CDC rows: {total_rows:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "equipment_state_header"
   },
   "source": [
    "## 7. Generate Equipment State Data\n",
    "\n",
    "Equipment state transitions at hourly intervals. During crisis:\n",
    "- LINE_4 has 40% chance of STARVATION state\n",
    "- Other lines have 20% chance of STARVATION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "generate_equipment_state"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GENERATE EQUIPMENT STATE LOG\n",
    "# =============================================================================\n",
    "# \n",
    "# Updated logic to create visible OEE-OPE gap:\n",
    "# - Normal period: 85% running, 10.5% baseline starvation, 4.5% other stops\n",
    "# - Crisis LINE_4: 60% starvation with 15-40 min duration\n",
    "# - Crisis other lines: 35% starvation with 10-20 min duration\n",
    "\n",
    "print(\"Generating EQUIPMENT_STATE_LOG_CDC...\")\n",
    "\n",
    "# Reset seed for reproducibility\n",
    "random.seed(RANDOM_SEED + 2)\n",
    "np.random.seed(RANDOM_SEED + 2)\n",
    "\n",
    "records = []\n",
    "current_date = START_DATE.date()\n",
    "\n",
    "while current_date <= END_DATE.date():\n",
    "    current_datetime = datetime.combine(current_date, datetime.min.time())\n",
    "    is_crisis = is_crisis_period(current_datetime)\n",
    "    \n",
    "    for line in PRODUCTION_LINES:\n",
    "        for hour in range(24):\n",
    "            event_time = current_datetime + timedelta(hours=hour)\n",
    "            shift_id = get_shift_for_hour(hour)\n",
    "            \n",
    "            if is_crisis and line == 'LINE_4':\n",
    "                # Crisis LINE_4: 60% starvation with 15-40 min duration (severe impact)\n",
    "                if random.random() < 0.60:\n",
    "                    state_code = 2\n",
    "                    state_name = 'IDLE'\n",
    "                    reason_code = 'STARVATION'\n",
    "                    reason_desc = 'Waiting for material - AGV delivery delayed due to sensor errors'\n",
    "                    duration = random.randint(900, 2400)  # 15-40 minutes\n",
    "                else:\n",
    "                    state_code = 1\n",
    "                    state_name = 'RUNNING'\n",
    "                    reason_code = None\n",
    "                    reason_desc = None\n",
    "                    duration = 3600\n",
    "            elif is_crisis:\n",
    "                # Crisis other lines: 35% starvation with 10-20 min duration\n",
    "                if random.random() < 0.35:\n",
    "                    state_code = 2\n",
    "                    state_name = 'IDLE'\n",
    "                    reason_code = 'STARVATION'\n",
    "                    reason_desc = 'Waiting for material - AGV delivery delayed'\n",
    "                    duration = random.randint(600, 1200)  # 10-20 minutes\n",
    "                else:\n",
    "                    state_code = 1\n",
    "                    state_name = 'RUNNING'\n",
    "                    reason_code = None\n",
    "                    reason_desc = None\n",
    "                    duration = 3600\n",
    "            else:\n",
    "                # Normal period: Add baseline starvation for realistic OEE-OPE gap\n",
    "                # 85% running, 10.5% baseline starvation, 4.5% other stops\n",
    "                rand_val = random.random()\n",
    "                if rand_val < 0.85:\n",
    "                    # 85% fully running\n",
    "                    state_code = 1\n",
    "                    state_name = 'RUNNING'\n",
    "                    reason_code = None\n",
    "                    reason_desc = None\n",
    "                    duration = 3600\n",
    "                elif rand_val < 0.955:\n",
    "                    # 10.5% baseline starvation (normal AGV cycle delays)\n",
    "                    state_code = 2\n",
    "                    state_name = 'IDLE'\n",
    "                    reason_code = 'STARVATION'\n",
    "                    reason_desc = 'Waiting for material - normal AGV cycle time'\n",
    "                    duration = random.randint(180, 600)  # 3-10 minutes\n",
    "                else:\n",
    "                    # 4.5% other minor stops\n",
    "                    state_code = random.choice([2, 3, 4])\n",
    "                    state_name = ['', 'RUNNING', 'IDLE', 'FAULT', 'SETUP'][state_code]\n",
    "                    reason_code = random.choice(['CHANGEOVER', 'MINOR_STOP', 'ADJUSTMENT'])\n",
    "                    reason_desc = 'Planned minor stop'\n",
    "                    duration = random.randint(60, 300)\n",
    "            \n",
    "            records.append({\n",
    "                'EVENT_ID': generate_uuid(),\n",
    "                'ASSET_ID': line,  # Must match ASSET_CODE in ATOMIC.ASSET for JOIN\n",
    "                'PRODUCTION_LINE_ID': line,\n",
    "                'EVENT_TIMESTAMP': event_time,\n",
    "                'STATE_CODE': state_code,\n",
    "                'STATE_NAME': state_name,\n",
    "                'REASON_CODE': reason_code,\n",
    "                'REASON_DESCRIPTION': reason_desc,\n",
    "                'DURATION_SECONDS': duration,\n",
    "                'SHIFT_ID': shift_id,\n",
    "                'OPERATOR_ID': f'OP_{random.randint(100, 999)}',\n",
    "                '_CDC_OPERATION': 'INSERT',\n",
    "                '_CDC_TIMESTAMP': event_time,\n",
    "                '_CDC_SEQUENCE': int(event_time.timestamp() * 1000),\n",
    "                '_CDC_SOURCE_SYSTEM': 'SIEMENS_MES'\n",
    "            })\n",
    "    \n",
    "    current_date += timedelta(days=1)\n",
    "\n",
    "df_equip = pd.DataFrame(records)\n",
    "write_to_snowflake(df_equip, 'EQUIPMENT_STATE_LOG_CDC', 'RAW')\n",
    "print(f\"âœ… Total EQUIPMENT_STATE_LOG_CDC rows: {len(df_equip):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "prod_orders_header"
   },
   "source": [
    "## 8. Generate Production Orders\n",
    "\n",
    "Production orders with realistic completion rates. **Key fix**: Includes ACTUAL_QTY column to show actual production vs target.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "generate_prod_orders"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GENERATE PRODUCTION ORDERS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Generating PROD_ORDER_HEADER_CDC...\")\n",
    "\n",
    "# Reset seed for reproducibility\n",
    "random.seed(RANDOM_SEED + 3)\n",
    "np.random.seed(RANDOM_SEED + 3)\n",
    "\n",
    "records = []\n",
    "order_id = 1000000\n",
    "current_date = START_DATE.date()\n",
    "\n",
    "while current_date <= END_DATE.date():\n",
    "    for line in PRODUCTION_LINES:\n",
    "        for shift in SHIFTS:\n",
    "            product = random.choice(PRODUCTS)\n",
    "            target_qty = random.randint(50, 200)\n",
    "            \n",
    "            current_datetime = datetime.combine(current_date, datetime.min.time())\n",
    "            is_crisis = is_crisis_period(current_datetime)\n",
    "            \n",
    "            # Determine completion rate based on crisis status\n",
    "            if is_crisis and line == 'LINE_4':\n",
    "                completion_rate = random.uniform(0.6, 0.7)  # 60-70% during crisis\n",
    "            elif is_crisis:\n",
    "                completion_rate = random.uniform(0.8, 0.9)  # 80-90% during crisis\n",
    "            else:\n",
    "                completion_rate = random.uniform(0.95, 1.0)  # 95-100% normal\n",
    "            \n",
    "            # Calculate actual quantity produced (KEY FIX)\n",
    "            actual_qty = int(target_qty * completion_rate)\n",
    "            \n",
    "            shift_start_hour = {'SHIFT_1': 6, 'SHIFT_2': 14, 'SHIFT_3': 22}[shift]\n",
    "            sched_start = datetime.combine(current_date, datetime.min.time()) + timedelta(hours=shift_start_hour)\n",
    "            sched_end = sched_start + timedelta(hours=8)\n",
    "            \n",
    "            records.append({\n",
    "                'ORDER_ID': f'PO{order_id}',\n",
    "                'ORDER_TYPE': 'PP01',\n",
    "                'SKU': product['sku'],\n",
    "                'TARGET_QTY': target_qty,\n",
    "                'ACTUAL_QTY': actual_qty,  # NEW: Store actual production\n",
    "                'UNIT_OF_MEASURE': 'EA',\n",
    "                'SCHED_START': sched_start,\n",
    "                'SCHED_END': sched_end,\n",
    "                'ACTUAL_START': sched_start,\n",
    "                'ACTUAL_END': sched_end if completion_rate > 0.9 else None,\n",
    "                'PRODUCTION_LINE_ID': line,\n",
    "                'WORK_CENTER_ID': f'WC_{line}',\n",
    "                'ORDER_STATUS': 'COMPLETED' if completion_rate > 0.9 else 'STARTED',\n",
    "                'PRIORITY': random.randint(1, 5),\n",
    "                '_CDC_OPERATION': 'INSERT',\n",
    "                '_CDC_TIMESTAMP': sched_start,\n",
    "                '_CDC_SEQUENCE': order_id,\n",
    "                '_CDC_SOURCE_SYSTEM': 'SAP_ERP'\n",
    "            })\n",
    "            \n",
    "            order_id += 1\n",
    "    \n",
    "    current_date += timedelta(days=1)\n",
    "\n",
    "df_orders = pd.DataFrame(records)\n",
    "write_to_snowflake(df_orders, 'PROD_ORDER_HEADER_CDC', 'RAW')\n",
    "print(f\"âœ… Total PROD_ORDER_HEADER_CDC rows: {len(df_orders):,}\")\n",
    "\n",
    "# Calculate and display crisis period metrics\n",
    "crisis_orders = df_orders[df_orders['ORDER_STATUS'] != 'COMPLETED']\n",
    "if len(crisis_orders) > 0:\n",
    "    avg_completion = crisis_orders['ACTUAL_QTY'].sum() / crisis_orders['TARGET_QTY'].sum() * 100\n",
    "    print(f\"   Crisis period avg completion rate: {avg_completion:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "material_docs_header"
   },
   "source": [
    "## 9. Generate Material Documents\n",
    "\n",
    "Inventory movements with increased quality inspection during crisis period.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "generate_material_docs"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GENERATE MATERIAL DOCUMENTS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Generating MATERIAL_DOCUMENT_CDC...\")\n",
    "\n",
    "# Reset seed for reproducibility\n",
    "random.seed(RANDOM_SEED + 4)\n",
    "np.random.seed(RANDOM_SEED + 4)\n",
    "\n",
    "records = []\n",
    "doc_id = 5000000\n",
    "current_date = START_DATE.date()\n",
    "\n",
    "while current_date <= END_DATE.date():\n",
    "    current_datetime = datetime.combine(current_date, datetime.min.time())\n",
    "    is_crisis = is_crisis_period(current_datetime)\n",
    "    \n",
    "    for _ in range(random.randint(50, 100)):\n",
    "        product = random.choice(PRODUCTS)\n",
    "        batch_id = f'BATCH_{random.randint(10000, 99999)}'\n",
    "        \n",
    "        if is_crisis and random.random() < 0.3:\n",
    "            stock_type = 'QUALITY_INSPECTION'\n",
    "            mvmt_type = '321'\n",
    "        else:\n",
    "            stock_type = 'UNRESTRICTED'\n",
    "            mvmt_type = random.choice(['101', '261', '601'])\n",
    "        \n",
    "        posting_time = current_datetime + timedelta(\n",
    "            hours=random.randint(0, 23),\n",
    "            minutes=random.randint(0, 59)\n",
    "        )\n",
    "        \n",
    "        records.append({\n",
    "            'MAT_DOC_ID': str(doc_id),\n",
    "            'POSTING_DATE': current_date,\n",
    "            'SKU': product['sku'],\n",
    "            'MVMT_TYPE': mvmt_type,\n",
    "            'STOCK_TYPE': stock_type,\n",
    "            'BATCH_ID': batch_id,\n",
    "            'PLANT_ID': 'P001',\n",
    "            'STORAGE_LOCATION': random.choice(['SL01', 'SL02', 'SL03']),\n",
    "            'QUANTITY': random.randint(10, 500),\n",
    "            'UNIT_OF_MEASURE': 'EA',\n",
    "            '_CDC_OPERATION': 'INSERT',\n",
    "            '_CDC_TIMESTAMP': posting_time,\n",
    "            '_CDC_SEQUENCE': doc_id,\n",
    "            '_CDC_SOURCE_SYSTEM': 'SAP_ERP'\n",
    "        })\n",
    "        \n",
    "        doc_id += 1\n",
    "    \n",
    "    current_date += timedelta(days=1)\n",
    "\n",
    "df_matdocs = pd.DataFrame(records)\n",
    "write_to_snowflake(df_matdocs, 'MATERIAL_DOCUMENT_CDC', 'RAW')\n",
    "print(f\"âœ… Total MATERIAL_DOCUMENT_CDC rows: {len(df_matdocs):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "verification_header"
   },
   "source": [
    "## 10. Verification\n",
    "\n",
    "Verify row counts and data quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "verify_data_generation"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VERIFICATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA GENERATION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Query row counts\n",
    "verification_sql = \"\"\"\n",
    "SELECT 'ATOMIC.ASSET' AS TABLE_NAME, COUNT(*) AS ROW_COUNT FROM ATOMIC.ASSET\n",
    "UNION ALL SELECT 'ATOMIC.PRODUCT', COUNT(*) FROM ATOMIC.PRODUCT\n",
    "UNION ALL SELECT 'RAW.ENV_SENSOR_STREAM_STAGE', COUNT(*) FROM RAW.ENV_SENSOR_STREAM_STAGE\n",
    "UNION ALL SELECT 'RAW.AGV_TELEMATICS_LOG_CDC', COUNT(*) FROM RAW.AGV_TELEMATICS_LOG_CDC\n",
    "UNION ALL SELECT 'RAW.EQUIPMENT_STATE_LOG_CDC', COUNT(*) FROM RAW.EQUIPMENT_STATE_LOG_CDC\n",
    "UNION ALL SELECT 'RAW.PROD_ORDER_HEADER_CDC', COUNT(*) FROM RAW.PROD_ORDER_HEADER_CDC\n",
    "UNION ALL SELECT 'RAW.MATERIAL_DOCUMENT_CDC', COUNT(*) FROM RAW.MATERIAL_DOCUMENT_CDC\n",
    "ORDER BY TABLE_NAME\n",
    "\"\"\"\n",
    "\n",
    "counts = session.sql(verification_sql).to_pandas()\n",
    "print(\"\\nðŸ“Š Row Counts:\")\n",
    "for _, row in counts.iterrows():\n",
    "    print(f\"   {row['TABLE_NAME']}: {row['ROW_COUNT']:,}\")\n",
    "\n",
    "# Verify crisis period data\n",
    "crisis_sql = \"\"\"\n",
    "SELECT \n",
    "    DATE(EVENT_TIMESTAMP) AS EVENT_DATE,\n",
    "    COUNT(*) AS TOTAL_READINGS,\n",
    "    SUM(CASE WHEN ERROR_CODE = 'AGV-ERR-99' THEN 1 ELSE 0 END) AS ERR_99_COUNT\n",
    "FROM RAW.AGV_TELEMATICS_LOG_CDC\n",
    "WHERE DATE(EVENT_TIMESTAMP) BETWEEN '2024-12-08' AND '2024-12-12'\n",
    "GROUP BY 1\n",
    "ORDER BY 1\n",
    "\"\"\"\n",
    "\n",
    "crisis_data = session.sql(crisis_sql).to_pandas()\n",
    "print(\"\\nðŸ”¥ Crisis Period AGV Errors (Dec 8-12):\")\n",
    "for _, row in crisis_data.iterrows():\n",
    "    print(f\"   {row['EVENT_DATE']}: {row['ERR_99_COUNT']:,} AGV-ERR-99 errors\")\n",
    "\n",
    "print(\"\\nâœ… Data generation complete! Run 03b_raw_to_atomic.sql to populate ATOMIC tables.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd4ab48",
   "metadata": {
    "name": "visualization_header"
   },
   "source": [
    "## 11. Data Quality Visualizations\n",
    "\n",
    "Visual verification that the crisis period patterns were generated correctly.\n",
    "These plots confirm the causal chain: Low Humidity â†’ High Dust â†’ AGV Errors â†’ Starvation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2509c4cc",
   "metadata": {
    "name": "setup_dark_theme"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VISUALIZATION SETUP - Snowflake Dark Theme\n",
    "# =============================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Apply dark theme\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams.update({\n",
    "    # Background colors (soft dark gray)\n",
    "    'figure.facecolor': '#121212',\n",
    "    'axes.facecolor': '#121212',\n",
    "    \n",
    "    # Text colors (off-white to reduce glare)\n",
    "    'text.color': '#E5E5E7',\n",
    "    'axes.labelcolor': '#E5E5E7',\n",
    "    'xtick.color': '#A1A1A6',\n",
    "    'ytick.color': '#A1A1A6',\n",
    "    \n",
    "    # Grid and axes (subtle)\n",
    "    'axes.edgecolor': '#3A3A3C',\n",
    "    'grid.color': '#2C2C2E',\n",
    "    'grid.alpha': 0.6,\n",
    "    \n",
    "    # Figure quality\n",
    "    'figure.dpi': 150,\n",
    "    'savefig.dpi': 200,\n",
    "    'figure.figsize': (14, 4),\n",
    "    \n",
    "    # Typography\n",
    "    'font.family': 'sans-serif',\n",
    "    'font.size': 10,\n",
    "    'axes.titlesize': 12,\n",
    "    'axes.labelsize': 10,\n",
    "})\n",
    "\n",
    "# Snowflake brand colors\n",
    "SNOWFLAKE_BLUE = '#29B5E8'\n",
    "MID_BLUE = '#11567F'\n",
    "VALENCIA_ORANGE = '#FF9F0A'\n",
    "ACCENT_CYAN = '#5AC8FA'\n",
    "\n",
    "print(\"âœ… Snowflake dark theme applied\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c289d57",
   "metadata": {
    "name": "visualize_crisis_patterns"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CRISIS PERIOD VERIFICATION PLOTS\n",
    "# =============================================================================\n",
    "\n",
    "# Query environmental data for crisis period (Dec 8-12)\n",
    "env_sql = \"\"\"\n",
    "SELECT \n",
    "    DATE(READING_TIMESTAMP) AS READING_DATE,\n",
    "    SENSOR_TYPE,\n",
    "    AVG(METRIC_VALUE) AS AVG_VALUE\n",
    "FROM RAW.ENV_SENSOR_STREAM_STAGE\n",
    "WHERE DATE(READING_TIMESTAMP) BETWEEN '2024-12-06' AND '2024-12-14'\n",
    "  AND SENSOR_TYPE IN ('HUMIDITY', 'PM25')\n",
    "GROUP BY 1, 2\n",
    "ORDER BY 1, 2\n",
    "\"\"\"\n",
    "env_df = session.sql(env_sql).to_pandas()\n",
    "\n",
    "# Query AGV errors\n",
    "agv_sql = \"\"\"\n",
    "SELECT \n",
    "    DATE(EVENT_TIMESTAMP) AS EVENT_DATE,\n",
    "    COUNT(*) AS TOTAL_READINGS,\n",
    "    SUM(CASE WHEN ERROR_CODE = 'AGV-ERR-99' THEN 1 ELSE 0 END) AS ERR_99_COUNT,\n",
    "    ROUND(100.0 * SUM(CASE WHEN ERROR_CODE = 'AGV-ERR-99' THEN 1 ELSE 0 END) / COUNT(*), 2) AS ERROR_RATE_PCT\n",
    "FROM RAW.AGV_TELEMATICS_LOG_CDC\n",
    "WHERE DATE(EVENT_TIMESTAMP) BETWEEN '2024-12-06' AND '2024-12-14'\n",
    "GROUP BY 1\n",
    "ORDER BY 1\n",
    "\"\"\"\n",
    "agv_df = session.sql(agv_sql).to_pandas()\n",
    "\n",
    "# Query production completion by line during crisis\n",
    "prod_sql = \"\"\"\n",
    "SELECT \n",
    "    PRODUCTION_LINE_ID,\n",
    "    CASE \n",
    "        WHEN DATE(SCHED_START) BETWEEN '2024-12-09' AND '2024-12-11' THEN 'Crisis'\n",
    "        ELSE 'Normal'\n",
    "    END AS PERIOD,\n",
    "    ROUND(100.0 * SUM(ACTUAL_QTY) / NULLIF(SUM(TARGET_QTY), 0), 1) AS COMPLETION_RATE\n",
    "FROM RAW.PROD_ORDER_HEADER_CDC\n",
    "WHERE DATE(SCHED_START) BETWEEN '2024-12-01' AND '2024-12-14'\n",
    "GROUP BY 1, 2\n",
    "ORDER BY 1, 2\n",
    "\"\"\"\n",
    "prod_df = session.sql(prod_sql).to_pandas()\n",
    "\n",
    "# Create 3-panel figure\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Panel 1: Environmental Conditions\n",
    "ax1 = axes[0]\n",
    "humidity_df = env_df[env_df['SENSOR_TYPE'] == 'HUMIDITY']\n",
    "dust_df = env_df[env_df['SENSOR_TYPE'] == 'PM25']\n",
    "\n",
    "ax1.plot(humidity_df['READING_DATE'], humidity_df['AVG_VALUE'], \n",
    "         color=SNOWFLAKE_BLUE, linewidth=2, marker='o', markersize=5, label='Humidity (%)')\n",
    "ax1_twin = ax1.twinx()\n",
    "ax1_twin.plot(dust_df['READING_DATE'], dust_df['AVG_VALUE'], \n",
    "              color=VALENCIA_ORANGE, linewidth=2, marker='s', markersize=5, label='Dust (Âµg/mÂ³)')\n",
    "\n",
    "# Crisis period shading\n",
    "ax1.axvspan(pd.Timestamp('2024-12-09'), pd.Timestamp('2024-12-11'), \n",
    "            alpha=0.2, color='red', label='Crisis Period')\n",
    "\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Humidity (%)', color=SNOWFLAKE_BLUE)\n",
    "ax1_twin.set_ylabel('Dust PM2.5 (Âµg/mÂ³)', color=VALENCIA_ORANGE)\n",
    "ax1.set_title('Environmental Conditions')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax1.legend(loc='upper left')\n",
    "ax1_twin.legend(loc='upper right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 2: AGV Error Rate\n",
    "ax2 = axes[1]\n",
    "bars = ax2.bar(agv_df['EVENT_DATE'], agv_df['ERROR_RATE_PCT'], \n",
    "               color=[VALENCIA_ORANGE if d >= pd.Timestamp('2024-12-09') and d <= pd.Timestamp('2024-12-11') \n",
    "                      else MID_BLUE for d in agv_df['EVENT_DATE']])\n",
    "ax2.set_xlabel('Date')\n",
    "ax2.set_ylabel('Error Rate (%)')\n",
    "ax2.set_title('AGV-ERR-99 Error Rate')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "ax2.axhline(y=1, color=ACCENT_CYAN, linestyle='--', alpha=0.5, label='Normal (<1%)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Panel 3: Production Completion by Line\n",
    "ax3 = axes[2]\n",
    "pivot_df = prod_df.pivot(index='PRODUCTION_LINE_ID', columns='PERIOD', values='COMPLETION_RATE').fillna(0)\n",
    "x = range(len(pivot_df))\n",
    "width = 0.35\n",
    "\n",
    "if 'Normal' in pivot_df.columns:\n",
    "    ax3.bar([i - width/2 for i in x], pivot_df['Normal'], width, \n",
    "            label='Normal Period', color=SNOWFLAKE_BLUE)\n",
    "if 'Crisis' in pivot_df.columns:\n",
    "    ax3.bar([i + width/2 for i in x], pivot_df['Crisis'], width, \n",
    "            label='Crisis Period', color=VALENCIA_ORANGE)\n",
    "\n",
    "ax3.set_xlabel('Production Line')\n",
    "ax3.set_ylabel('Completion Rate (%)')\n",
    "ax3.set_title('Production Completion Rate')\n",
    "ax3.set_xticks(x)\n",
    "ax3.set_xticklabels(pivot_df.index)\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "ax3.set_ylim(0, 110)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/tmp/crisis_verification.png', dpi=150, bbox_inches='tight', \n",
    "            facecolor='#121212', edgecolor='none')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Visualization complete - crisis patterns verified\")\n",
    "print(\"   ðŸ“‰ Humidity dropped from ~45% to ~25% during crisis\")\n",
    "print(\"   ðŸ“ˆ Dust spiked from ~10 to ~35 Âµg/mÂ³\")  \n",
    "print(\"   âš ï¸  AGV-ERR-99 rate increased to ~15% during crisis\")\n",
    "print(\"   ðŸ­ LINE_4 completion dropped to ~65% during crisis\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d9a4b7",
   "metadata": {
    "name": "key_takeaways"
   },
   "source": [
    "## 12. Key Takeaways\n",
    "\n",
    "### Generated Data Summary\n",
    "\n",
    "| Data Type | Volume | Crisis Impact |\n",
    "|-----------|--------|---------------|\n",
    "| Environmental sensors | ~1.5M rows | Humidity 45% â†’ 25%, Dust 10 â†’ 35 Âµg/mÂ³ |\n",
    "| AGV telemetry | ~2.6M rows | 15% error rate during crisis (vs <1% normal) |\n",
    "| Equipment states | ~11K rows | 60% starvation on LINE_4 during crisis |\n",
    "| Production orders | ~1.4K rows | 60-70% completion during crisis |\n",
    "\n",
    "### Causal Chain Demonstrated\n",
    "\n",
    "```\n",
    "Low Humidity (25%) â†’ High Dust (35 Âµg/mÂ³) â†’ AGV-ERR-99 â†’ Line Starvation â†’ OPE Drop (60%)\n",
    "```\n",
    "\n",
    "This chain demonstrates the **OEE vs OPE gap**: \n",
    "- OEE measures equipment availability (unaffected by AGV issues)\n",
    "- OPE measures operational performance (includes material flow dependencies)\n",
    "\n",
    "### Interpretation Guidelines\n",
    "\n",
    "| Metric | Normal Range | Crisis Range | Interpretation |\n",
    "|--------|--------------|--------------|----------------|\n",
    "| Humidity | 40-50% | 20-30% | Low humidity indicates HVAC issues or dry weather |\n",
    "| PM2.5 Dust | 5-15 Âµg/mÂ³ | 30-40 Âµg/mÂ³ | High dust causes optical sensor failures |\n",
    "| AGV Error Rate | <1% | 10-20% | ERR-99 indicates sensor obscuration |\n",
    "| Completion Rate | 95-100% | 60-80% | Low completion = material starvation |\n",
    "\n",
    "### Limitations\n",
    "\n",
    "1. **Simplified Causal Model**: Real manufacturing has more complex failure modes\n",
    "2. **Concentrated Crisis**: 3-day window (Dec 9-11) for demo clarity\n",
    "3. **Uniform AGV Behavior**: All AGVs affected equally during crisis\n",
    "4. **No Cascading Effects**: LINE_4 is independently worst-affected\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Run `03b_raw_to_atomic.sql` to populate ATOMIC layer tables\n",
    "2. Execute OPE calculation views to compute metrics\n",
    "3. Use the Streamlit dashboard to visualize the OEE-OPE gap\n",
    "4. Train anomaly detection models on this labeled dataset\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
